{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfpWq1bDR00q6UcXo2IvPL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaYunJeong/K-Digital-Training-Class/blob/main/9.%20NLP/Aug06_2_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing - 텍스트 전처리\n",
        "\n",
        "내가 해결하고자 하는 문제의 용도에 맞게 텍스트를 사전에 처리해버리는 작업"
      ],
      "metadata": {
        "id": "9_JMT8e5E391"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAab9BuJEnb_",
        "outputId": "18afcf03-edda-406b-8ae2-41493cf35f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk   # 자연어(사람들이 사용하는 언어)를 처리 하기 위한 패키지\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "nltk.download('punkt')    # 문장 구조를 학습한 일종의 모델\n",
        "# 2 ~ 4번줄 문장을 토큰화 해주기 위한 import"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화(Tokenization)\n",
        "\n",
        "어떤 문장을 단어로 잘라내서 정제하고, 정규화를 시키는 과정\n",
        "\n",
        "- 구두점(Functuation)\n",
        "  * 마침표, 쉼표, 물음표, 느낌표, 세미콜론, ..."
      ],
      "metadata": {
        "id": "5hqhZXfiF2YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The beginning is half of the whole.  Easy come, easy go. Do as you would be done by. Make hay while the sun shines.\""
      ],
      "metadata": {
        "id": "_tu7_K2yFd3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))\n",
        "# word_tokenize : Don't => Do와 n't / you're => you와 're로 구분\n",
        "print()\n",
        "\n",
        "print(WordPunctTokenizer().tokenize(text))\n",
        "# 구두점을 별로도 표시 : Don't => Don 와 ' 와 t\n",
        "print()\n",
        "\n",
        "print(text_to_word_sequence(text))\n",
        "# keras의 text_to_word_sequence : 모든 알파벳을 소문자로 바꿔줌\n",
        "#                                 구두점 제거\n",
        "#                                 you're , don't, ain't 같은 경우는 그대로 보존함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYbNZAm3HBov",
        "outputId": "0b191181-380f-4494-9897-699e9affa0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'beginning', 'is', 'half', 'of', 'the', 'whole', '.', 'Easy', 'come', ',', 'easy', 'go', '.', 'Do', 'as', 'you', 'would', 'be', 'done', 'by', '.', 'Make', 'hay', 'while', 'the', 'sun', 'shines', '.']\n",
            "\n",
            "['The', 'beginning', 'is', 'half', 'of', 'the', 'whole', '.', 'Easy', 'come', ',', 'easy', 'go', '.', 'Do', 'as', 'you', 'would', 'be', 'done', 'by', '.', 'Make', 'hay', 'while', 'the', 'sun', 'shines', '.']\n",
            "\n",
            "['the', 'beginning', 'is', 'half', 'of', 'the', 'whole', 'easy', 'come', 'easy', 'go', 'do', 'as', 'you', 'would', 'be', 'done', 'by', 'make', 'hay', 'while', 'the', 'sun', 'shines']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문장 토큰화(Sentence Tokenization)\n",
        "\n",
        "http://gutenberg.org/ (외국 소설책 사이트)"
      ],
      "metadata": {
        "id": "z4CMgpR1NFX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\"\"It was on seeing that boy that I understood, for the first time, my situation. I had thought up to that moment of the adventures before me, not at all of the home that I was leaving; and now, at sight of this clumsy stranger, who was to stay here in my place beside my mother, I had my first attack of tears. I am afraid I led that boy a dog’s life, for as he was new to the work, I had a hundred opportunities of setting him right and putting him down, and I was not slow to profit by them.\"\"\"\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "GqEMZo7XHXmd",
        "outputId": "7e71c8e2-3b16-4afa-f9bc-2ffcf291d59b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It was on seeing that boy that I understood, for the first time, my situation. I had thought up to that moment of the adventures before me, not at all of the home that I was leaving; and now, at sight of this clumsy stranger, who was to stay here in my place beside my mother, I had my first attack of tears. I am afraid I led that boy a dog’s life, for as he was new to the work, I had a hundred opportunities of setting him right and putting him down, and I was not slow to profit by them.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "1e7pMe_7N8o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(sentence)\n",
        "# NLTK는 단순하게 마침표로 문장을 구분하지 않음\n",
        "# Dr. , Mrs. , Mr. 등 단어들은 마침표를 기준으로 해서 나뉘어지지 않음 => 성공적 !!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMUkYFrGOH9W",
        "outputId": "da48760a-7461-4dc8-ce11-acdd78580ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It was on seeing that boy that I understood, for the first time, my situation.',\n",
              " 'I had thought up to that moment of the adventures before me, not at all of the home that I was leaving; and now, at sight of this clumsy stranger, who was to stay here in my place beside my mother, I had my first attack of tears.',\n",
              " 'I am afraid I led that boy a dog’s life, for as he was new to the work, I had a hundred opportunities of setting him right and putting him down, and I was not slow to profit by them.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KSS (Korean Sentence Splitter)\n",
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfs6jRGtONSH",
        "outputId": "913d71eb-49a8-433c-c7d6-61484a7e65a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-6.0.4.tar.gz (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji==1.2.0 (from kss)\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pecab (from kss)\n",
            "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.3)\n",
            "Collecting jamo (from kss)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting hangul-jamo (from kss)\n",
            "  Downloading hangul_jamo-1.0.1-py3-none-any.whl.metadata (899 bytes)\n",
            "Collecting tossi (from kss)\n",
            "  Downloading tossi-0.3.1.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting distance (from kss)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyyaml==6.0 (from kss)\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting unidecode (from kss)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting cmudict (from kss)\n",
            "  Downloading cmudict-1.0.27-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting koparadigm (from kss)\n",
            "  Downloading koparadigm-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting kollocate (from kss)\n",
            "  Downloading kollocate-0.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting bs4 (from kss)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from kss) (1.26.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from kss) (7.4.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from kss) (1.13.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->kss) (4.12.3)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict->kss) (8.2.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict->kss) (6.4.0)\n",
            "Collecting whoosh (from kollocate->kss)\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting xlrd==1.2.0 (from koparadigm->kss)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (14.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (2024.5.15)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (24.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (2.0.1)\n",
            "Requirement already satisfied: bidict in /usr/local/lib/python3.10/dist-packages (from tossi->kss) (0.23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tossi->kss) (1.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=5->cmudict->kss) (3.19.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->kss) (2.5)\n",
            "Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading cmudict-1.0.27-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hangul_jamo-1.0.1-py3-none-any.whl (4.4 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading kollocate-0.0.2-py3-none-any.whl (72.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading koparadigm-0.10.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kss, distance, pecab, tossi\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-6.0.4-cp310-cp310-linux_x86_64.whl size=1446482 sha256=f397ecaa5f4d6af4b9ab395e1ee6e0b833309135cfeeceabbe995b84a2c0eba6\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/70/d5/c9308346829b1eb9e7267d74696919d2453aee6ce350f98b3b\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16257 sha256=688f832bad89fb8442f2e8622043f8896dc722e2b9c93627f3f156dd30fa8f88\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646664 sha256=c61ae97ce25fd3ea9aa4678e041e9753dc31421860c4d5e89858141ad77f312f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/6f/b4/ab61b8863d7d8b1409def8ae31adcaa089fa91b8d022ec309d\n",
            "  Building wheel for tossi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tossi: filename=tossi-0.3.1-py3-none-any.whl size=12128 sha256=83bd9d6c559225ab22be9f6b56549663b1a65424dfe78595c5aa215c5940a52c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/18/60/1094a6fe93c8063efcd3e6700d09328216682e495a3c51af9f\n",
            "Successfully built kss distance pecab tossi\n",
            "Installing collected packages: whoosh, jamo, hangul-jamo, emoji, distance, xlrd, unidecode, tossi, pyyaml, kollocate, pecab, koparadigm, cmudict, bs4, kss\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "Successfully installed bs4-0.0.2 cmudict-1.0.27 distance-0.1.3 emoji-1.2.0 hangul-jamo-1.0.1 jamo-0.4.1 kollocate-0.0.2 koparadigm-0.10.0 kss-6.0.4 pecab-1.0.8 pyyaml-6.0 tossi-0.3.1 unidecode-1.3.8 whoosh-2.7.4 xlrd-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss"
      ],
      "metadata": {
        "id": "sJ6j66onPCEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kor = \"오늘부터 AI 시작이에요. 텍스트 전처리는 한국어가 영어보다 훨씬 난이도가 높아요. 한번 경험해봅시다.\"\n",
        "kor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6XekqtpPQPko",
        "outputId": "b547f9de-c868-4def-cf9a-186cc6674a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'오늘부터 AI 시작이에요. 텍스트 전처리는 한국어가 영어보다 훨씬 난이도가 높아요. 한번 경험해봅시다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(kss.split_sentences(kor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY7wlNRKQot4",
        "outputId": "be566076-05a3-4c5e-fbb6-88ec3b0ddff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['오늘부터 AI 시작이에요.', '텍스트 전처리는 한국어가 영어보다 훨씬 난이도가 높아요.', '한번 경험해봅시다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어 = 교착어(어근 + 접사)\n",
        "\n",
        "한국어에는 [조사]가 존재\n",
        "\n",
        "- 글자 뒤에 띄어쓰기 없이 존재\n",
        "- 형태소 (morpheme)\n",
        "  - 말의 가장 작은 단위\n",
        "    - 자립형태소 : 스스로 자립하여 쓰일 수 있는\n",
        "              명사, 대명사, 수사, 관형사, 부사, ...\n",
        "    - 의존형태소 : 다른 형태소와 결합을 해야만하는\n",
        "              어간, 어미, 접사, 조사, ..."
      ],
      "metadata": {
        "id": "XN6yO0y0QyDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 품사 태깅(Part-of-speech tagging) : 단어 토큰화를 거친 토큰들(단어들)에게 품사를 붙여주는 작업\n",
        "\n",
        "동음이의어\n",
        "\n",
        "mean : 동사] 의미하다 / 명사] 평균 / 형용사] 비열한, 못된\n",
        "\n",
        "연패 : 연속해서 패하다 / 연속해서 이기다"
      ],
      "metadata": {
        "id": "-Jr5vxznRvAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK / KoNLPy"
      ],
      "metadata": {
        "id": "iExjgg5fSYVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')   # 품사태깅을 위한 라이브러리"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GskRYJqUQrrv",
        "outputId": "68d487bf-f388-4c95-c493-8f16666ce6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag"
      ],
      "metadata": {
        "id": "InlO9gKFSkTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The beginning is half of the whole.  Easy come, easy go. Do as you would be done by. Make hay while the sun shines.\"\n",
        "tokenized_sentence = word_tokenize(text)\n",
        "print(tokenized_sentence)\n",
        "print(pos_tag(tokenized_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxxflgs8Sq53",
        "outputId": "3678efef-32b9-403a-911c-edaab10f2c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'beginning', 'is', 'half', 'of', 'the', 'whole', '.', 'Easy', 'come', ',', 'easy', 'go', '.', 'Do', 'as', 'you', 'would', 'be', 'done', 'by', '.', 'Make', 'hay', 'while', 'the', 'sun', 'shines', '.']\n",
            "[('The', 'DT'), ('beginning', 'NN'), ('is', 'VBZ'), ('half', 'DT'), ('of', 'IN'), ('the', 'DT'), ('whole', 'NN'), ('.', '.'), ('Easy', 'NNP'), ('come', 'VBP'), (',', ','), ('easy', 'JJ'), ('go', 'NN'), ('.', '.'), ('Do', 'VB'), ('as', 'IN'), ('you', 'PRP'), ('would', 'MD'), ('be', 'VB'), ('done', 'VBN'), ('by', 'IN'), ('.', '.'), ('Make', 'NNP'), ('hay', 'NN'), ('while', 'IN'), ('the', 'DT'), ('sun', 'NN'), ('shines', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRP : 인칭대명사\n",
        "# RB : 부사\n",
        "# DT : 관사\n",
        "# VBP : 단수, 현재형, 3인칭 동사\n",
        "# W ~ : wh~\n",
        "# JJ : 형용사\n",
        "# NN : 단수명사\n",
        "# NNS : 복수명사\n",
        "# MD : 조동사\n",
        "# VB : 동사 기본형\n",
        "# VBD : 동사 과거시제\n",
        "# VBG : 동명사\n"
      ],
      "metadata": {
        "id": "Rqd5b211TUMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어 자연어처리 : koNLPy라는 파이썬 패키지\n",
        "\n",
        "KoNLPy에서 사용할 수 있는 형태소 분석기\n",
        "- Okt (Open Korean Text)\n",
        "- Komoran\n",
        "- kkma(꼬꼬마)\n",
        "- Mecab\n",
        "- Hannanum"
      ],
      "metadata": {
        "id": "57YilTtLT7XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZSrfid1TBwp",
        "outputId": "2cc5f1b7-20a4-4684-a207-fbde9aff3529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma"
      ],
      "metadata": {
        "id": "x3faHSWJUXsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "print(okt.morphs(\"오늘은 화요일이고요. 내일은 수요일입니다!\"))\n",
        "# morphs : 형태소 분석 : 어떤 대상의 어절을 최소 의미단위인 형태소로 분석하는 것\n",
        "print(okt.pos(\"오늘은 화요일이고요. 내일은 수요일입니다!\"))\n",
        "# pos : 품사 태깅(Part-of-Speech tagging)\n",
        "print(okt.nouns(\"오늘은 화요일이고요. 내일은 수요일입니다!\"))\n",
        "# nouns : 명사 추출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Un3JC7XUkeJ",
        "outputId": "43d869e0-8fe8-44b2-e1d9-3ad9d7e29169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['오늘', '은', '화요일', '이', '고요', '.', '내일', '은', '수요일', '입니다', '!']\n",
            "[('오늘', 'Noun'), ('은', 'Josa'), ('화요일', 'Noun'), ('이', 'Josa'), ('고요', 'Noun'), ('.', 'Punctuation'), ('내일', 'Noun'), ('은', 'Josa'), ('수요일', 'Noun'), ('입니다', 'Adjective'), ('!', 'Punctuation')]\n",
            "['오늘', '화요일', '고요', '내일', '수요일']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma = Kkma()\n",
        "\n",
        "print(kkma.morphs(\"오늘은 화요일이고요. 내일은 수요일입니다!\"))\n",
        "print(kkma.pos(\"오늘은 화요일이고요. 내일은 수요일입니다!\"))\n",
        "print(kkma.nouns(\"오늘은 화요일이고요. 내일은 수요일입니다!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKjEo9S2U61Y",
        "outputId": "b5b5bc6b-79ef-4f5c-adc3-520ce496f21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['오늘', '은', '화요일', '이', '고요', '.', '내일', '은', '수요일', '이', 'ㅂ니다', '!']\n",
            "[('오늘', 'NNG'), ('은', 'JX'), ('화요일', 'NNG'), ('이', 'VCP'), ('고요', 'EFN'), ('.', 'SF'), ('내일', 'NNG'), ('은', 'JX'), ('수요일', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('!', 'SF')]\n",
            "['오늘', '화요일', '내일', '수요일']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 코퍼스(Coupus) : 말뭉치\n",
        "\n",
        "보통 여러 단어들로 이루어진 문장, 분석하려는 대상, 문서, 데이터셋\n",
        "\n",
        "코퍼스에서 용도에 맞게 토큰을 나누는 것을 토큰화(Tokenization)\n",
        "\n",
        "토큰화를 진행하기 전, 후에 텍스트를 용도에 맞게 정제(Cleaning), 정규화(Normalization)를 하는 것이 필요 ! (전처리 , 후처리)\n",
        "\n",
        "- 정제(Cleaning) : 가지고 있는 말뭉치에서 노이즈 데이터를 제거\n",
        "    => 의미가 없는 조사, 접사들을 없애는 작업\n",
        "- 정규화(Normalization) : 표현 방법이 서로 다른 단어들을 통일시켜서 같은 단어로 재가공\n",
        "    1. 규칙에 따라 표기가 다른 언어를 통합시키기\n",
        "      ex) US USA us U.S.A\n",
        "    2. 대소문자를 통합\n",
        "    \n"
      ],
      "metadata": {
        "id": "5Uh48oqWWnuq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "har8e5p0Vnh5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}